\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{indentfirst}

\begin{document}
\thispagestyle{empty}
\begin{center}
	\textbf{\huge{Advanced Computer Vision \textemdash~Review: 1/26}} \\
	\bigskip
	\large{Murat Ambarkutuk \{murata@vt.edu\}}
\end{center}
\section{Fisher kernels on visual vocabularies for image categorization}
%%% Summary
% By its ill-posed nature of Computer Vision, trying to extract information from the images
Image categorization is the effort of extracting semantic level information from images and labeling them based on the extracted information.
%, is one of the problems investigated by many researchers in Computer Vision field.
% Along with its ill-posed nature of any arbitrary Computer Vision problem, variations in many aspects of the images make image categorization problem one of the open problems.
In their paper~\cite{perronnin2007fisher}, the authors propose an approach in which Fisher Kernels (FK) and Gaussian Mixture Models (GMM) are utilizied to achieve image categorization problems.
By doing so, the authors provide a unified framework for two different kinds of pattern classification methods, namely, generative and discriminative approaches.
%, which form the basis of one of the contributions.
The intuition is to approximate low-level feature vectors extracted from the images with Gaussian functions, while representing them with FK.\@ \\
%%% Approach
%% Describe the approach taken
%% Emphasize the contributions
% The approach is inspired by the idea of Bag-of-Words representation. So the whole framework can be understood as a
\indent The proposed approach contains two major steps in constructing the vocabulary: the approximation of the low-level feature vectors with a number of Gaussian functions, and the representation of the derived Gaussian mixtures with the gradients of log-likelihood functions.
The first contribution of this paper is to employ the pipeline of GMM, and FK in the image categorization problems to provide a combined framework for two seperate approaches of categorization.
Each N-dimensional feature vector is formulated as the sum of log-likelihood functions given the a set number of Gaussian functions.
One can object the idea of representing any arbitrary distribution with a set number of Gaussian functions, given that one may end up having highly non-Gaussian (multi-modal) distributions.
Given the influence of this parameter in the whole process, choosing a constant is not trivial.
% The occupancy probability is then calculated with Bayes' formula to figure out which feature vector was generated by which Gaussian function.
The second step of the approach is to calculate the gradient vectors of the log-likelihood functions of feature vectors.
% Being approximated with Gaussians, the partial derivatives are taken w.r.t.\ the weight, the mean and the covariance of the Gaussians.
Along with utilization of FK in image categorization problems, the derivation of the closed-form formulation of the Fisher information matrix is the second contribution of the paper.
% The images, first, is resized to the approximately the same size, from which approximately the same number of features are extracted.
The images were preprocessed before extracting features by resizing the images approximately to the same size.
After that, the dimensionality of the feature vectors are reduced to 50.
In the training (unsupervised) step Maximum Likelihood estimation was used to construct ``universal vocabulary''\cite{perronnin2007fisher}, from which the class-vocabulary is established by using Maximum a Posteirori estimation. \\
%%% Results
%% Explaint experimental setup
%% Underline the results
\indent The proposed method was compherensively tested with two different databases, in house and VOC 2006~\cite{voc-2006}, under supervised and unsupervised learning schemes with two different low-level feature vectors to validate the derived approach.
Both of the dataset has more than 5000 images, for testing and training combined for less than 20 categories.
Thus, it can be inferable that FK is a tailored solution for particular problems, given the need of amount of effort and data in the training process. 
The results provided by the authors validate the the overall performance of FK\@.
Showing at least 10\% better performance than median of the VOC 2006 applicants, FK seems to prove its success in categorization problems,  which may be resulted from having higher dimensionality in representing the words.
Along with its relatively higher succes, the computational efficacy of FK is higher by the factor of 25 than the conventional BOV representation, albeit the aforementioned dimensionality.

\bibliographystyle{unsrt}
\bibliography{bibliography}
\end{document}
